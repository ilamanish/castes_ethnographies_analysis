{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an NER Model for the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the docs\n",
    "\n",
    "The following code loads the descriptions of each caste as doc entities from the pickled file created earlier. It then filters the descriptions to retain only those that are more than a sentence long. It then saves these to a dataframe with the relevant doc ID <span style='color:red'>(but the spaCy doc entities are converted to strings in the CSV file)</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the stored file of doc entities\n",
    "with open(\"/Users/ilamanish/Documents/dh_ism/scripts/data/descriptions_docs.pkl\", \"rb\") as descriptions_docs:\n",
    "    docs = pickle.load(descriptions_docs)\n",
    "\n",
    "# Loading the indices and corresponding doc entities for all descriptions longer than one sentence (hereafter referred to as long descriptions)\n",
    "long_descriptions = []\n",
    "ids = []\n",
    "\n",
    "for doc_index, doc in enumerate(docs):\n",
    "    sent_counter = 0\n",
    "    for sent in doc.sents:\n",
    "        sent_counter += 1\n",
    "    if sent_counter > 1:\n",
    "        ids.append(doc_index)\n",
    "        long_descriptions.append(doc)\n",
    "\n",
    "# Saving the list of sentences and corresponding doc indices as a csv\n",
    "long_descriptions_df = pd.DataFrame({'doc_id': ids, 'Description': long_descriptions})\n",
    "long_descriptions_df.to_csv('./data/long_descriptions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Sentences\n",
    "\n",
    "Sentences are individually extracted and saved to enable sentence-based analysis and modelling. A set of 1000 sample sentences is then extracted for manual annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caste</th>\n",
       "      <th>Text Description</th>\n",
       "      <th>Tokenized Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acchu Tāli</td>\n",
       "      <td>A sub-division of Vāniyan.</td>\n",
       "      <td>(A, sub, -, division, of, Vāniyan, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acchu Tāli</td>\n",
       "      <td>The name refers to the peculiar tāli (marriage...</td>\n",
       "      <td>(The, name, refers, to, the, peculiar, tāli, (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acchuvāru</td>\n",
       "      <td>Recorded, in the Madras Census Report, 1901, a...</td>\n",
       "      <td>(Recorded, ,, in, the, Madras, Census, Report,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acchuvāru</td>\n",
       "      <td>Treated as a sub-division of Gaudo.”</td>\n",
       "      <td>(Treated, as, a, sub, -, division, of, Gaudo, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acchuvāru</td>\n",
       "      <td>The Acchuvārus are not Oriya people, but are a...</td>\n",
       "      <td>(The, Acchuvārus, are, not, Oriya, people, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41866</th>\n",
       "      <td>Yōgi Gurukkal</td>\n",
       "      <td>It is recorded, in the Gazetteer of Malabar, t...</td>\n",
       "      <td>(It, is, recorded, ,, in, the, Gazetteer, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41867</th>\n",
       "      <td>Yōgi Gurukkal</td>\n",
       "      <td>They perform sakti pūja in their own houses, t...</td>\n",
       "      <td>(They, perform, sakti, pūja, in, their, own, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41868</th>\n",
       "      <td>Yōgi Gurukkal</td>\n",
       "      <td>They are celebrated sorcerers and exorcists, a...</td>\n",
       "      <td>(They, are, celebrated, sorcerers, and, exorci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41869</th>\n",
       "      <td>Zonnala</td>\n",
       "      <td>Zonnala, or the equivalent Zonnakūti, has been...</td>\n",
       "      <td>(Zonnala, ,, or, the, equivalent, Zonnakūti, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41870</th>\n",
       "      <td>Zonnala</td>\n",
       "      <td>The Kōyis hold a festival when the zonna crop ...</td>\n",
       "      <td>(The, Kōyis, hold, a, festival, when, the, zon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41871 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Caste                                   Text Description  \\\n",
       "0         Acchu Tāli                         A sub-division of Vāniyan.   \n",
       "1         Acchu Tāli  The name refers to the peculiar tāli (marriage...   \n",
       "2          Acchuvāru  Recorded, in the Madras Census Report, 1901, a...   \n",
       "3          Acchuvāru               Treated as a sub-division of Gaudo.”   \n",
       "4          Acchuvāru  The Acchuvārus are not Oriya people, but are a...   \n",
       "...              ...                                                ...   \n",
       "41866  Yōgi Gurukkal  It is recorded, in the Gazetteer of Malabar, t...   \n",
       "41867  Yōgi Gurukkal  They perform sakti pūja in their own houses, t...   \n",
       "41868  Yōgi Gurukkal  They are celebrated sorcerers and exorcists, a...   \n",
       "41869        Zonnala  Zonnala, or the equivalent Zonnakūti, has been...   \n",
       "41870        Zonnala  The Kōyis hold a festival when the zonna crop ...   \n",
       "\n",
       "                                   Tokenized Description  \n",
       "0                  (A, sub, -, division, of, Vāniyan, .)  \n",
       "1      (The, name, refers, to, the, peculiar, tāli, (...  \n",
       "2      (Recorded, ,, in, the, Madras, Census, Report,...  \n",
       "3      (Treated, as, a, sub, -, division, of, Gaudo, ...  \n",
       "4      (The, Acchuvārus, are, not, Oriya, people, ,, ...  \n",
       "...                                                  ...  \n",
       "41866  (It, is, recorded, ,, in, the, Gazetteer, of, ...  \n",
       "41867  (They, perform, sakti, pūja, in, their, own, h...  \n",
       "41868  (They, are, celebrated, sorcerers, and, exorci...  \n",
       "41869  (Zonnala, ,, or, the, equivalent, Zonnakūti, ,...  \n",
       "41870  (The, Kōyis, hold, a, festival, when, the, zon...  \n",
       "\n",
       "[41871 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "sent_tokens_list = []\n",
    "sent_text_list = []\n",
    "sent_indice_list = []\n",
    "\n",
    "for doc_id, doc in long_descriptions_df[['doc_id', 'Description']].iterrows():\n",
    "    for sent in doc['Description'].sents:\n",
    "        sent_tokens_list.append(sent)\n",
    "        sent_text_list.append(sent.text)\n",
    "        sent_indice_list.append(doc['doc_id'])\n",
    "\n",
    "castes_dataframe = pd.read_csv('./data/castes_dataframe.csv')\n",
    "castes = []\n",
    "for i in sent_indice_list:\n",
    "    castes.append(castes_dataframe.iloc[i,1])\n",
    "    \n",
    "column_names = ['Caste', 'Text Description', 'Tokenized Description']\n",
    "sentences_df = pd.DataFrame(list(zip(castes, sent_text_list, sent_tokens_list)),\n",
    "                  columns = column_names)\n",
    "\n",
    "display(sentences_df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "sentences_df.to_csv('./data/all_sentences.csv', index=False)\n",
    "\n",
    "# Extract 1000 random sentences for manual annotation\n",
    "sample_sentences = random.sample(sentences_df['Text Description'].tolist(), k=1000)\n",
    "\n",
    "# Save the sample sentences as a txt file\n",
    "with open('./data/sample_sentences.txt', 'w') as file:\n",
    "    # Write each element of the list to the file\n",
    "    for value in sample_sentences:\n",
    "        file.write(str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the NER Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training data\n",
    "\n",
    "The following code uses manual annotations of **material entities** and **social relations** across 1000 sentences from the corpus to create the requisite training data. The annotation was done on tecoholic's NER Annotator: https://tecoholic.github.io/ner-annotator/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the requisite libraries\n",
    "import json\n",
    "\n",
    "# Loading the annotations from the exported JSON file\n",
    "with open('./data/annotations.json', 'r') as file:\n",
    "    TRAIN_DATA = json.load(file)\n",
    "\n",
    "print(len(TRAIN_DATA))\n",
    "print(TRAIN_DATA[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the NER Model\n",
    "Code adapted from https://ner.pythonhumanities.com/03_02_train_spacy_ner_model.html \n",
    "\n",
    "The following code converts the training data into the binary format required in spaCy 3. It then splits the training data into a training set and a validation set and saves these in the right formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import srsly\n",
    "import typer\n",
    "import spacy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Converting the training dataset into spaCy 3 binary format\n",
    "def convert(lang: str, TRAIN_DATA, output_path: Path):\n",
    "    nlp = spacy.blank(lang)\n",
    "    db = DocBin()\n",
    "    for text, annot in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            # Skipping over data that is not in the required format\n",
    "            if span is None:\n",
    "                msg = f\"Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\\n\\n{repr(text)}\\n\"\n",
    "                warnings.warn(msg)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(output_path)\n",
    "\n",
    "# Determining the proportion of data for the training set and validation set\n",
    "train_proportion = 0.8  # Adjust this value based on your preference\n",
    "\n",
    "# Calculating the index to split the data\n",
    "split_index = int(len(TRAIN_DATA) * train_proportion)\n",
    "\n",
    "# Splitting the data\n",
    "train_data = TRAIN_DATA[:split_index]\n",
    "valid_data = TRAIN_DATA[split_index:]\n",
    "\n",
    "# Performing spaCy conversion\n",
    "convert(\"en\", train_data, \"/Users/ilamanish/Documents/dh_ism/scripts/data/train.spacy\")\n",
    "convert(\"en\", valid_data, \"/Users/ilamanish/Documents/dh_ism/scripts/data/valid.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a config.cfg file using the base_config.cfg file created using spaCy GUI: https://spacy.io/usage/training#quickstart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting the base_config.cfg file into a properly formatted config.cfg file using spaCy\n",
    "\n",
    "!python3 -m spacy init fill-config /Users/ilamanish/Documents/dh_ism/scripts/data/base_config.cfg data/config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains the NER model using the training and validation sets created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the spaCy model\n",
    "\n",
    "!python3 -m spacy train data/config.cfg --output ./models/material_models/output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
